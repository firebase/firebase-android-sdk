/*
 * Copyright 2024 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.firebase.ai

import android.graphics.Bitmap
import com.google.firebase.FirebaseApp
import com.google.firebase.ai.common.APIController
import com.google.firebase.ai.common.AppCheckHeaderProvider
import com.google.firebase.ai.generativemodel.CloudGenerativeModelProvider
import com.google.firebase.ai.generativemodel.FallbackGenerativeModelProvider
import com.google.firebase.ai.generativemodel.GenerativeModelProvider
import com.google.firebase.ai.generativemodel.MissingOnDeviceGenerativeModelProvider
import com.google.firebase.ai.generativemodel.OnDeviceGenerativeModelProvider
import com.google.firebase.ai.ondevice.interop.FirebaseAIOnDeviceGenerativeModelFactory
import com.google.firebase.ai.type.AutoFunctionDeclaration
import com.google.firebase.ai.type.Content
import com.google.firebase.ai.type.CountTokensResponse
import com.google.firebase.ai.type.FirebaseAIException
import com.google.firebase.ai.type.FirebaseAutoFunctionException
import com.google.firebase.ai.type.FunctionCallPart
import com.google.firebase.ai.type.FunctionResponsePart
import com.google.firebase.ai.type.GenerateContentResponse
import com.google.firebase.ai.type.GenerateObjectResponse
import com.google.firebase.ai.type.GenerationConfig
import com.google.firebase.ai.type.GenerativeBackend
import com.google.firebase.ai.type.InvalidStateException
import com.google.firebase.ai.type.JsonSchema
import com.google.firebase.ai.type.RequestOptions
import com.google.firebase.ai.type.SafetySetting
import com.google.firebase.ai.type.Tool
import com.google.firebase.ai.type.ToolConfig
import com.google.firebase.ai.type.content
import com.google.firebase.appcheck.interop.InteropAppCheckTokenProvider
import com.google.firebase.auth.internal.InternalAuthProvider
import kotlinx.coroutines.flow.Flow
import kotlinx.serialization.InternalSerializationApi
import kotlinx.serialization.json.Json
import kotlinx.serialization.json.JsonObject
import kotlinx.serialization.json.JsonPrimitive
import kotlinx.serialization.json.jsonObject

/**
 * Represents a multimodal model (like Gemini), capable of generating content based on various input
 * types.
 */
public class GenerativeModel
internal constructor(
  private val tools: List<Tool>? = null,
  private val actualModel: GenerativeModelProvider,
  internal val controller: APIController,
) {
  /**
   * Generates new content from the input [Content] given to the model as a prompt.
   *
   * @param prompt The input(s) given to the model as a prompt.
   * @return The content generated by the model.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun generateContent(prompt: List<Content>): GenerateContentResponse =
    actualModel.generateContent(prompt)

  /**
   * Generates new content from the input [Content] given to the model as a prompt.
   *
   * @param prompt The input(s) given to the model as a prompt.
   * @return The content generated by the model.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun generateContent(
    prompt: Content,
    vararg prompts: Content
  ): GenerateContentResponse = generateContent(listOf(prompt, *prompts))

  /**
   * Generates new content from the text input given to the model as a prompt.
   *
   * @param prompt The text to be send to the model as a prompt.
   * @return The content generated by the model.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun generateContent(prompt: String): GenerateContentResponse =
    generateContent(listOf(content { text(prompt) }))

  /**
   * Generates new content from the image input given to the model as a prompt.
   *
   * @param prompt The image to be converted into a single piece of [Content] to send to the model.
   * @return A [GenerateContentResponse] after some delay.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun generateContent(prompt: Bitmap): GenerateContentResponse =
    generateContent(listOf(content { image(prompt) }))

  /**
   * Generates new content as a stream from the input [Content] given to the model as a prompt.
   *
   * @param prompt The input(s) given to the model as a prompt.
   * @return A [Flow] which will emit responses as they are returned by the model.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public fun generateContentStream(prompt: List<Content>): Flow<GenerateContentResponse> =
    actualModel.generateContentStream(prompt)

  /**
   * Generates new content as a stream from the input [Content] given to the model as a prompt.
   *
   * @param prompt The input(s) given to the model as a prompt.
   * @return A [Flow] which will emit responses as they are returned by the model.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public fun generateContentStream(
    prompt: Content,
    vararg prompts: Content
  ): Flow<GenerateContentResponse> = generateContentStream(listOf(prompt, *prompts))

  /**
   * Generates new content as a stream from the text input given to the model as a prompt.
   *
   * @param prompt The text to be send to the model as a prompt.
   * @return A [Flow] which will emit responses as they are returned by the model.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public fun generateContentStream(prompt: String): Flow<GenerateContentResponse> =
    generateContentStream(listOf(content { text(prompt) }))

  /**
   * Generates new content as a stream from the image input given to the model as a prompt.
   *
   * @param prompt The image to be converted into a single piece of [Content] to send to the model.
   * @return A [Flow] which will emit responses as they are returned by the model.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public fun generateContentStream(prompt: Bitmap): Flow<GenerateContentResponse> =
    generateContentStream(listOf(content { image(prompt) }))

  /**
   * Generates an object from the input [Content] given to the model as a prompt.
   *
   * @param jsonSchema A schema for the output
   * @param prompt The input(s) given to the model as a prompt.
   * @return The content generated by the model.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun <T : Any> generateObject(
    jsonSchema: JsonSchema<T>,
    prompt: Content,
    vararg prompts: Content
  ): GenerateObjectResponse<T> = actualModel.generateObject(jsonSchema, listOf(prompt, *prompts))

  /**
   * Generates an object from the text input given to the model as a prompt.
   *
   * @param jsonSchema A schema for the output
   * @param prompt The text to be send to the model as a prompt.
   * @return The content generated by the model.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun <T : Any> generateObject(
    jsonSchema: JsonSchema<T>,
    prompt: String
  ): GenerateObjectResponse<T> = generateObject(jsonSchema, content { text(prompt) })

  /** Creates a [Chat] instance using this model with the optionally provided history. */
  public fun startChat(history: List<Content> = emptyList()): Chat =
    Chat(this, history.toMutableList())

  /**
   * Counts the number of tokens in a prompt using the model's tokenizer.
   *
   * @param prompt The input(s) given to the model as a prompt.
   * @return The [CountTokensResponse] of running the model's tokenizer on the input.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun countTokens(prompt: List<Content>): CountTokensResponse =
    actualModel.countTokens(prompt)

  /**
   * Counts the number of tokens in a prompt using the model's tokenizer.
   *
   * @param prompt The input(s) given to the model as a prompt.
   * @return The [CountTokensResponse] of running the model's tokenizer on the input.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun countTokens(prompt: Content, vararg prompts: Content): CountTokensResponse =
    countTokens(listOf(prompt, *prompts))

  /**
   * Counts the number of tokens in a text prompt using the model's tokenizer.
   *
   * @param prompt The text given to the model as a prompt.
   * @return The [CountTokensResponse] of running the model's tokenizer on the input.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun countTokens(prompt: String): CountTokensResponse =
    countTokens(listOf(content { text(prompt) }))

  /**
   * Counts the number of tokens in an image prompt using the model's tokenizer.
   *
   * @param prompt The image given to the model as a prompt.
   * @return The [CountTokensResponse] of running the model's tokenizer on the input.
   * @throws [FirebaseAIException] if the request failed.
   * @see [FirebaseAIException] for types of errors.
   */
  public suspend fun countTokens(prompt: Bitmap): CountTokensResponse =
    countTokens(listOf(content { image(prompt) }))

  /**
   * Warms up the model to reduce latency for the first request.
   *
   * @throws [FirebaseAIException] if the warmup failed.
   */
  public suspend fun warmUp(): Unit = actualModel.warmUp()

  internal fun hasFunction(call: FunctionCallPart): Boolean {
    return tools
      ?.flatMap { it.autoFunctionDeclarations?.filterNotNull() ?: emptyList() }
      ?.firstOrNull { it.name == call.name && it.functionReference != null } != null
  }

  @OptIn(InternalSerializationApi::class)
  internal suspend fun executeFunction(call: FunctionCallPart): FunctionResponsePart {
    if (tools == null) {
      throw RuntimeException("No registered tools")
    }
    val tool = tools.flatMap { it.autoFunctionDeclarations?.filterNotNull() ?: emptyList() }
    val declaration =
      tool.firstOrNull() { it.name == call.name }
        ?: throw RuntimeException("No registered function named ${call.name}")
    return executeFunction<Any, Any>(
      call,
      declaration as AutoFunctionDeclaration<Any, Any>,
      JsonObject(call.args).toString()
    )
  }

  @OptIn(InternalSerializationApi::class)
  internal suspend fun <I : Any, O : Any> executeFunction(
    functionCall: FunctionCallPart,
    functionDeclaration: AutoFunctionDeclaration<I, O>,
    parameter: String
  ): FunctionResponsePart {
    val inputDeserializer = functionDeclaration.inputSchema.getSerializer()
    val input = Json.decodeFromString(inputDeserializer, parameter)
    val functionReference =
      functionDeclaration.functionReference
        ?: throw RuntimeException("Function reference for ${functionDeclaration.name} is missing")
    try {
      val output = functionReference.invoke(input)
      val outputSerializer = functionDeclaration.outputSchema?.getSerializer()
      if (outputSerializer != null) {
        return FunctionResponsePart.from(
            Json.encodeToJsonElement(outputSerializer, output).jsonObject
          )
          .normalizeAgainstCall(functionCall)
      }
      return (output as FunctionResponsePart).normalizeAgainstCall(functionCall)
    } catch (e: FirebaseAutoFunctionException) {
      return FunctionResponsePart.from(JsonObject(mapOf("error" to JsonPrimitive(e.message))))
        .normalizeAgainstCall(functionCall)
    }
  }

  internal fun getTurnLimit(): Int = controller.getTurnLimit()

  internal companion object {
    private val TAG = GenerativeModel::class.java.simpleName

    internal fun create(
      modelName: String,
      apiKey: String,
      firebaseApp: FirebaseApp,
      useLimitedUseAppCheckTokens: Boolean,
      generationConfig: GenerationConfig? = null,
      safetySettings: List<SafetySetting>? = null,
      tools: List<Tool>? = null,
      toolConfig: ToolConfig? = null,
      systemInstruction: Content? = null,
      requestOptions: RequestOptions = RequestOptions(),
      onDeviceConfig: OnDeviceConfig,
      generativeBackend: GenerativeBackend,
      appCheckTokenProvider: InteropAppCheckTokenProvider? = null,
      onDeviceFactoryProvider: FirebaseAIOnDeviceGenerativeModelFactory? = null,
      internalAuthProvider: InternalAuthProvider? = null,
    ): GenerativeModel {
      val apiController =
        APIController(
          apiKey,
          modelName,
          requestOptions,
          "gl-kotlin/${KotlinVersion.CURRENT}-ai fire/${BuildConfig.VERSION_NAME}",
          firebaseApp,
          AppCheckHeaderProvider(
            TAG,
            useLimitedUseAppCheckTokens,
            appCheckTokenProvider,
            internalAuthProvider
          ),
        )

      val cloudModel =
        CloudGenerativeModelProvider(
          modelName = modelName,
          generationConfig = generationConfig,
          safetySettings = safetySettings,
          tools = tools,
          toolConfig = toolConfig,
          systemInstruction = systemInstruction,
          generativeBackend = generativeBackend,
          controller = apiController,
        )
      val onDeviceModel =
        if (onDeviceFactoryProvider == null) {
          MissingOnDeviceGenerativeModelProvider()
        } else {
          OnDeviceGenerativeModelProvider(
            onDeviceFactoryProvider.newGenerativeModel(),
            onDeviceConfig
          )
        }

      val actualModelProvider =
        when (onDeviceConfig.mode) {
          InferenceMode.ONLY_IN_CLOUD -> cloudModel
          InferenceMode.ONLY_ON_DEVICE -> onDeviceModel
          InferenceMode.PREFER_ON_DEVICE ->
            FallbackGenerativeModelProvider(
              defaultModel = onDeviceModel,
              fallbackModel = cloudModel,
              shouldFallbackInException = true
            )
          InferenceMode.PREFER_IN_CLOUD ->
            FallbackGenerativeModelProvider(
              defaultModel = cloudModel,
              fallbackModel = onDeviceModel,
              // TODO: Add check for precondition (network should be available)
              shouldFallbackInException = false
            )
          else -> throw InvalidStateException("Invalid inference mode")
        }

      return GenerativeModel(
        tools = tools,
        actualModel = actualModelProvider,
        controller = apiController,
      )
    }
  }
}
