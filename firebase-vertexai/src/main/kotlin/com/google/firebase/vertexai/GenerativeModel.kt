/*
 * Copyright 2024 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.firebase.vertexai

import android.graphics.Bitmap
import android.util.Log
import com.google.firebase.appcheck.interop.InteropAppCheckTokenProvider
import com.google.firebase.auth.internal.InternalAuthProvider
import com.google.firebase.vertexai.common.APIController
import com.google.firebase.vertexai.common.CountTokensRequest
import com.google.firebase.vertexai.common.GenerateContentRequest
import com.google.firebase.vertexai.common.HeaderProvider
import com.google.firebase.vertexai.internal.util.toInternal
import com.google.firebase.vertexai.internal.util.toPublic
import com.google.firebase.vertexai.type.Content
import com.google.firebase.vertexai.type.CountTokensResponse
import com.google.firebase.vertexai.type.FinishReason
import com.google.firebase.vertexai.type.FirebaseVertexAIException
import com.google.firebase.vertexai.type.GenerateContentResponse
import com.google.firebase.vertexai.type.GenerationConfig
import com.google.firebase.vertexai.type.PromptBlockedException
import com.google.firebase.vertexai.type.RequestOptions
import com.google.firebase.vertexai.type.ResponseStoppedException
import com.google.firebase.vertexai.type.SafetySetting
import com.google.firebase.vertexai.type.SerializationException
import com.google.firebase.vertexai.type.Tool
import com.google.firebase.vertexai.type.ToolConfig
import com.google.firebase.vertexai.type.content
import kotlin.time.Duration
import kotlin.time.Duration.Companion.seconds
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.catch
import kotlinx.coroutines.flow.map
import kotlinx.coroutines.tasks.await

/**
 * Represents a multimodal model (like Gemini), capable of generating content based on various input
 * types.
 */
public class GenerativeModel
internal constructor(
  private val modelName: String,
  private val generationConfig: GenerationConfig? = null,
  private val safetySettings: List<SafetySetting>? = null,
  private val tools: List<Tool>? = null,
  private val toolConfig: ToolConfig? = null,
  private val systemInstruction: Content? = null,
  private val controller: APIController
) {

  @JvmOverloads
  internal constructor(
    modelName: String,
    apiKey: String,
    generationConfig: GenerationConfig? = null,
    safetySettings: List<SafetySetting>? = null,
    tools: List<Tool>? = null,
    toolConfig: ToolConfig? = null,
    systemInstruction: Content? = null,
    requestOptions: RequestOptions = RequestOptions(),
    appCheckTokenProvider: InteropAppCheckTokenProvider? = null,
    internalAuthProvider: InternalAuthProvider? = null,
  ) : this(
    modelName,
    generationConfig,
    safetySettings,
    tools,
    toolConfig,
    systemInstruction,
    APIController(
      apiKey,
      modelName,
      requestOptions,
      "gl-kotlin/${KotlinVersion.CURRENT} fire/${BuildConfig.VERSION_NAME}",
      object : HeaderProvider {
        override val timeout: Duration
          get() = 10.seconds

        override suspend fun generateHeaders(): Map<String, String> {
          val headers = mutableMapOf<String, String>()
          if (appCheckTokenProvider == null) {
            Log.w(TAG, "AppCheck not registered, skipping")
          } else {
            val token = appCheckTokenProvider.getToken(false).await()

            if (token.error != null) {
              Log.w(TAG, "Error obtaining AppCheck token", token.error)
            }
            // The Firebase App Check backend can differentiate between apps without App Check, and
            // wrongly configured apps by verifying the value of the token, so it always needs to be
            // included.
            headers["X-Firebase-AppCheck"] = token.token
          }

          if (internalAuthProvider == null) {
            Log.w(TAG, "Auth not registered, skipping")
          } else {
            try {
              val token = internalAuthProvider.getAccessToken(false).await()

              headers["Authorization"] = "Firebase ${token.token!!}"
            } catch (e: Exception) {
              Log.w(TAG, "Error getting Auth token ", e)
            }
          }

          return headers
        }
      }
    )
  )

  /**
   * Generates new content from the input [Content] given to the model as a prompt.
   *
   * @param prompt The input(s) given to the model as a prompt.
   * @return The content generated by the model.
   * @throws [FirebaseVertexAIException] if the request failed.
   * @see [FirebaseVertexAIException] for types of errors.
   */
  public suspend fun generateContent(vararg prompt: Content): GenerateContentResponse =
    try {
      controller.generateContent(constructRequest(*prompt)).toPublic().validate()
    } catch (e: Throwable) {
      throw FirebaseVertexAIException.from(e)
    }

  /**
   * Generates new content as a stream from the input [Content] given to the model as a prompt.
   *
   * @param prompt The input(s) given to the model as a prompt.
   * @return A [Flow] which will emit responses as they are returned by the model.
   * @throws [FirebaseVertexAIException] if the request failed.
   * @see [FirebaseVertexAIException] for types of errors.
   */
  public fun generateContentStream(vararg prompt: Content): Flow<GenerateContentResponse> =
    controller
      .generateContentStream(constructRequest(*prompt))
      .catch { throw FirebaseVertexAIException.from(it) }
      .map { it.toPublic().validate() }

  /**
   * Generates new content from the text input given to the model as a prompt.
   *
   * @param prompt The text to be send to the model as a prompt.
   * @return The content generated by the model.
   * @throws [FirebaseVertexAIException] if the request failed.
   * @see [FirebaseVertexAIException] for types of errors.
   */
  public suspend fun generateContent(prompt: String): GenerateContentResponse =
    generateContent(content { text(prompt) })

  /**
   * Generates new content as a stream from the text input given to the model as a prompt.
   *
   * @param prompt The text to be send to the model as a prompt.
   * @return A [Flow] which will emit responses as they are returned by the model.
   * @throws [FirebaseVertexAIException] if the request failed.
   * @see [FirebaseVertexAIException] for types of errors.
   */
  public fun generateContentStream(prompt: String): Flow<GenerateContentResponse> =
    generateContentStream(content { text(prompt) })

  /**
   * Generates new content from the image input given to the model as a prompt.
   *
   * @param prompt The image to be converted into a single piece of [Content] to send to the model.
   * @return A [GenerateContentResponse] after some delay.
   * @throws [FirebaseVertexAIException] if the request failed.
   * @see [FirebaseVertexAIException] for types of errors.
   */
  public suspend fun generateContent(prompt: Bitmap): GenerateContentResponse =
    generateContent(content { image(prompt) })

  /**
   * Generates new content as a stream from the image input given to the model as a prompt.
   *
   * @param prompt The image to be converted into a single piece of [Content] to send to the model.
   * @return A [Flow] which will emit responses as they are returned by the model.
   * @throws [FirebaseVertexAIException] if the request failed.
   * @see [FirebaseVertexAIException] for types of errors.
   */
  public fun generateContentStream(prompt: Bitmap): Flow<GenerateContentResponse> =
    generateContentStream(content { image(prompt) })

  /** Creates a [Chat] instance using this model with the optionally provided history. */
  public fun startChat(history: List<Content> = emptyList()): Chat =
    Chat(this, history.toMutableList())

  /**
   * Counts the number of tokens in a prompt using the model's tokenizer.
   *
   * @param prompt The input(s) given to the model as a prompt.
   * @return The [CountTokensResponse] of running the model's tokenizer on the input.
   * @throws [FirebaseVertexAIException] if the request failed.
   * @see [FirebaseVertexAIException] for types of errors.
   */
  public suspend fun countTokens(vararg prompt: Content): CountTokensResponse {
    try {
      return controller.countTokens(constructCountTokensRequest(*prompt)).toPublic()
    } catch (e: Throwable) {
      throw FirebaseVertexAIException.from(e)
    }
  }

  /**
   * Counts the number of tokens in a text prompt using the model's tokenizer.
   *
   * @param prompt The text given to the model as a prompt.
   * @return The [CountTokensResponse] of running the model's tokenizer on the input.
   * @throws [FirebaseVertexAIException] if the request failed.
   * @see [FirebaseVertexAIException] for types of errors.
   */
  public suspend fun countTokens(prompt: String): CountTokensResponse {
    return countTokens(content { text(prompt) })
  }

  /**
   * Counts the number of tokens in an image prompt using the model's tokenizer.
   *
   * @param prompt The image given to the model as a prompt.
   * @return The [CountTokensResponse] of running the model's tokenizer on the input.
   * @throws [FirebaseVertexAIException] if the request failed.
   * @see [FirebaseVertexAIException] for types of errors.
   */
  public suspend fun countTokens(prompt: Bitmap): CountTokensResponse {
    return countTokens(content { image(prompt) })
  }

  private fun constructRequest(vararg prompt: Content) =
    GenerateContentRequest(
      modelName,
      prompt.map { it.toInternal() },
      safetySettings?.map { it.toInternal() },
      generationConfig?.toInternal(),
      tools?.map { it.toInternal() },
      toolConfig?.toInternal(),
      systemInstruction?.copy(role = "system")?.toInternal()
    )

  private fun constructCountTokensRequest(vararg prompt: Content) =
    CountTokensRequest.forVertexAI(constructRequest(*prompt))

  private fun GenerateContentResponse.validate() = apply {
    if (candidates.isEmpty() && promptFeedback == null) {
      throw SerializationException("Error deserializing response, found no valid fields")
    }
    promptFeedback?.blockReason?.let { throw PromptBlockedException(this) }
    candidates
      .mapNotNull { it.finishReason }
      .firstOrNull { it != FinishReason.STOP }
      ?.let { throw ResponseStoppedException(this) }
  }

  private companion object {
    private val TAG = GenerativeModel::class.java.simpleName
  }
}
